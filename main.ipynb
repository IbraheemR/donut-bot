{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import ray\n",
    "from ray.rllib.algorithms import ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DonutGame:\n",
    "#     def __init__(self):\n",
    "#         self.deck = list(range(13)) * 4\n",
    "\n",
    "#         shuffle(self.deck)        \n",
    "\n",
    "#         self.players = [\n",
    "#             {\n",
    "#                 \"visible\": [self.deck.pop(), self.deck.pop()],\n",
    "#                 \"visbleLockedIn\": [False, False],\n",
    "#                 \"hidden\": [self.deck.pop(), self.deck.pop()],\n",
    "#                 \"hiddenLockedIn\": [False, False],\n",
    "#                 \"drawnCard\": -1\n",
    "#             },\n",
    "#             {\n",
    "#                 \"visible\": [self.deck.pop(), self.deck.pop()],\n",
    "#                 \"visbleLockedIn\": [False, False],\n",
    "#                 \"hidden\": [self.deck.pop(), self.deck.pop()],\n",
    "#                 \"hiddenLockedIn\": [False, False],\n",
    "#                 \"drawnCard\": -1\n",
    "#             },\n",
    "#                         {\n",
    "#                 \"visible\": [self.deck.pop(), self.deck.pop()],\n",
    "#                 \"visbleLockedIn\": [False, False],\n",
    "#                 \"hidden\": [self.deck.pop(), self.deck.pop()],\n",
    "#                 \"hiddenLockedIn\": [False, False],\n",
    "#                 \"drawnCard\": -1\n",
    "#             },\n",
    "#                         {\n",
    "#                 \"visible\": [self.deck.pop(), self.deck.pop()],\n",
    "#                 \"visbleLockedIn\": [False, False],\n",
    "#                 \"hidden\": [self.deck.pop(), self.deck.pop()],\n",
    "#                 \"hiddenLockedIn\": [False, False],\n",
    "#                 \"drawnCard\": -1\n",
    "#             }\n",
    "#         ]\n",
    "\n",
    "#         self.burnedCards = []\n",
    "\n",
    "#         self.currentPlayer = 0\n",
    "#         self._stage = 0\n",
    "\n",
    "#     def game_finished(self):\n",
    "#         return all(all(p[\"visbleLockedIn\"]) and all(p[\"hiddenLockedIn\"]) for p in self.players)\n",
    "\n",
    "#     def getCurrentCards(self):\n",
    "#         player = self.players[self.currentPlayer].copy()\n",
    "\n",
    "        \n",
    "#         hiddenMasked = []\n",
    "\n",
    "#         for h, l in zip(player[\"hidden\"], player[\"hiddenLockedIn\"]):\n",
    "#             hiddenMasked.append(h if l else -1)\n",
    "\n",
    "#         player[\"hidden\"] = hiddenMasked    \n",
    "\n",
    "#         player[\"pile\"] = self.burnedCards[-1] if self._stage == 0 else -1\n",
    "\n",
    "#         return player\n",
    "\n",
    "#     def drawCard(self, chooseBurnPile):\n",
    "#         \"Choose to draw a card either from the random deck (False) or the known card (True)\"\n",
    "        \n",
    "#         assert self._stage == 0\n",
    "#         self._stage = 1\n",
    "\n",
    "#         player = self.players[self.currentPlayer]\n",
    "\n",
    "\n",
    "#         if chooseBurnPile:\n",
    "#             player[\"drawnCard\"] = self.deck.pop()\n",
    "#         else:\n",
    "#             player[\"drawnCard\"] = self.burnedCards.pop()\n",
    "        \n",
    "#     def playCard(self, cardPos, shouldSwap):\n",
    "#         \"Choose which position (0-3) to flip over and whether to use the drawn card or the one in the pile currently\"\n",
    "        \n",
    "#         assert self._stage == 1\n",
    "#         self._stage = 0\n",
    "\n",
    "#         player = self.players[self.currentPlayer]\n",
    "\n",
    "#         arr = None\n",
    "#         arrLockedIn = None\n",
    "\n",
    "\n",
    "#         if cardPos < 2:\n",
    "#             arr = player[\"visible\"]\n",
    "#             arrLockedIn = player[\"visibleLockedIn\"]\n",
    "#         else:\n",
    "#             arr = player[\"hidden\"]\n",
    "#             arrLockedIn = player[\"hiddenLockedIn\"]\n",
    "\n",
    "#         arrLockedIn[cardPos % 2] = True\n",
    "\n",
    "#         if shouldSwap:\n",
    "#             arr[cardPos % 2] = player[\"drawnCard\"]\n",
    "#         else\n",
    "#             self.burnedCards.append(player[\"drawnCard\"])\n",
    "        \n",
    "#         player[\"drawnCard\"] = -1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinglePlayerDonutGame:\n",
    "    VISIBILITY_MASK = (\n",
    "        True,\n",
    "        True,\n",
    "        False,\n",
    "        False\n",
    "    )\n",
    "\n",
    "    def __init__(self):\n",
    "        self.deck = list(range(13)) * 4\n",
    "        shuffle(self.deck)\n",
    "\n",
    "        self.cards = [\n",
    "            self.deck.pop(),\n",
    "            self.deck.pop(),\n",
    "            self.deck.pop(),\n",
    "            self.deck.pop()\n",
    "        ]\n",
    "\n",
    "        self.tempDrawnCard = -1\n",
    "\n",
    "        self.discardPile = [self.deck.pop()]\n",
    "\n",
    "        self.lockedCards = [\n",
    "            False,\n",
    "            False,\n",
    "            False,\n",
    "            False\n",
    "        ]\n",
    "\n",
    "        self.stage = 0 # 0 = drawing 1 = playing\n",
    "    \n",
    "    def isFinished(self):\n",
    "        return all(self.lockedCards)\n",
    "\n",
    "    def observe(self):\n",
    "        cards = []\n",
    "        for card, isVisible, isLocked in zip(self.cards, self.VISIBILITY_MASK, self.lockedCards):\n",
    "            cards.append(card if isVisible or isLocked else -1)\n",
    "\n",
    "        return {\n",
    "            \"cards\": cards,\n",
    "            \"lockedCard\": self.lockedCards,\n",
    "            \"tempDrawnCard\": self.tempDrawnCard if self.stage == 1 else -1,\n",
    "            \"discardPileTopCard\": self.discardPile[-1] if self.stage == 0 else -1,\n",
    "            \"stage\": self.stage\n",
    "        }\n",
    "\n",
    "    def drawCard(self, isFromRandomPile):\n",
    "        assert self.stage == 0\n",
    "        self.stage = 1\n",
    "\n",
    "        if isFromRandomPile:\n",
    "            self.tempDrawnCard = self.deck.pop()\n",
    "        else:\n",
    "            self.tempDrawnCard = self.discardPile.pop()\n",
    "\n",
    "        return \n",
    "\n",
    "    def playCard(self, position, shouldSwap):\n",
    "        if self.lockedCards[position]: return -1 # to\n",
    "\n",
    "        assert self.stage == 1\n",
    "        self.stage = 0\n",
    "\n",
    "\n",
    "        if shouldSwap:\n",
    "            self.discardPile.append(self.cards[position])\n",
    "            self.cards[position] = self.tempDrawnCard\n",
    "            self.tempDrawnCard = -1\n",
    "\n",
    "        else:\n",
    "            self.discardPile.append(self.tempDrawnCard)\n",
    "            self.tempDrawnCard = -1\n",
    "\n",
    "        self.lockedCards[position] = True\n",
    "        \n",
    "        return 0\n",
    "\n",
    "    def getFinalScore(self):\n",
    "        # 0 = King\n",
    "        # 1 = Ace\n",
    "        # 2 = 2\n",
    "        # ...\n",
    "        # 10 = 10\n",
    "        # 11 = J\n",
    "        # 12 = Q\n",
    "\n",
    "        assert self.isFinished()\n",
    "\n",
    "        finalCards = self.cards.copy()\n",
    "\n",
    "        score = 0\n",
    "\n",
    "        while len(finalCards):\n",
    "            c = finalCards.pop()\n",
    "\n",
    "            if c in finalCards:\n",
    "                finalCards.remove(c) # Remove one othe occurence of c to consume the pair - if there is 3 c's then it should count as 1 pair and 1 single\n",
    "            else:\n",
    "                score += c\n",
    "                \n",
    "        return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_observation(obs):\n",
    "    # some fields can value -1 for invalid or hidden. Map these to +ve intager indexes\n",
    "    return {\n",
    "        **obs,\n",
    "        \"cards\": [c + 1 for c in obs[\"cards\"]],\n",
    "        \"tempDrawnCard\": obs[\"tempDrawnCard\"] + 1,\n",
    "        \"discardPileTopCard\": obs[\"discardPileTopCard\"] + 1\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinglePlayerDonutEnv(gym.Env):\n",
    "    def __init__(self, env_config={}):\n",
    "        self.action_space = spaces.Dict({\n",
    "            \"drawFromRandom\": spaces.Discrete(2),\n",
    "\n",
    "            \"position\": spaces.MultiDiscrete(4),\n",
    "            \"shouldSwap\": spaces.Discrete(2)\n",
    "        })\n",
    "\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"cards\": spaces.MultiDiscrete([14, 14, 14, 14]),\n",
    "            \"lockedCard\": spaces.MultiBinary(4),\n",
    "            \"tempDrawnCard\": spaces.Discrete(14),\n",
    "            \"discardPileTopCard\": spaces.Discrete(14),\n",
    "            \"stage\": spaces.Discrete(2)\n",
    "        })\n",
    "\n",
    "        self.game = SinglePlayerDonutGame()        \n",
    "\n",
    "    def reset(self):\n",
    "        self.game = SinglePlayerDonutGame()\n",
    "\n",
    "        return map_observation(self.game.observe())\n",
    "        \n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "\n",
    "        if self.game.stage == 0:\n",
    "            self.game.drawCard(action[\"drawFromRandom\"])\n",
    "            \n",
    "        elif self.game.stage == 1:\n",
    "            e = self.game.playCard(\n",
    "                action[\"position\"],\n",
    "                action[\"shouldSwap\"]\n",
    "            )\n",
    "\n",
    "            reward = 10 * e\n",
    "\n",
    "        obs = self.game.observe()\n",
    "        done = self.game.isFinished()\n",
    "\n",
    "        if done:\n",
    "            reward = -self.game.getFinalScore()\n",
    "\n",
    "            if reward == 0: # Lots more reward for donut. Should really do a proper test with this.\n",
    "                reward = 10\n",
    "\n",
    "        return map_observation(obs), reward, done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_with_synthetic_deck(fake_deck, correct_score):\n",
    "    f = SinglePlayerDonutEnv()\n",
    "    f.reset()\n",
    "\n",
    "\n",
    "    f.game.cards = [fake_deck.pop() for i in range(4)]\n",
    "    f.game.discardPile = [fake_deck.pop()]\n",
    "\n",
    "    f.game.deck = fake_deck\n",
    "\n",
    "    f.step({\"drawFromRandom\": 1})\n",
    "    f.step({\n",
    "        \"position\": 0,\n",
    "        \"shouldSwap\": 0\n",
    "    })\n",
    "    f.step({\"drawFromRandom\": 1})\n",
    "    f.step({\n",
    "        \"position\": 1,\n",
    "        \"shouldSwap\": 0\n",
    "    })\n",
    "    f.step({\"drawFromRandom\": 1})\n",
    "    f.step({\n",
    "        \"position\": 2,\n",
    "        \"shouldSwap\": 0\n",
    "    })\n",
    "    f.step({\"drawFromRandom\": 1})\n",
    "    _, reward, *_ = f.step({\n",
    "        \"position\": 3,\n",
    "        \"shouldSwap\": 0\n",
    "    })\n",
    "\n",
    "    assert reward == correct_score, f\"Got reward {reward} but expected {correct_score} with final cards {f.game.cards}\"\n",
    "\n",
    "\n",
    "test_with_synthetic_deck([5, 5, 5, 5, 5, 7, 8, 9, 0], -7-8-9) # Dummy test deck to check no matches\n",
    "test_with_synthetic_deck([5, 5, 5, 5, 5, 7, 7, 8, 9], -8-9) # Dummy test deck to check a double\n",
    "test_with_synthetic_deck([5, 5, 5, 5, 5, 7, 7, 8, 8], 10) # Dummy test deck to check 2 doubles\n",
    "test_with_synthetic_deck([5, 5, 5, 5, 5, 7, 7, 7, 8], -7-8) # Dummy test deck to check a triple\n",
    "test_with_synthetic_deck([5, 5, 5, 5, 5, 7, 7, 7, 7], 10) # Dummy test deck to check a quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_invalid_position():\n",
    "    f = SinglePlayerDonutEnv()\n",
    "    f.reset()\n",
    "\n",
    "    f.step({\"drawFromRandom\": 1})\n",
    "    f.step({\n",
    "        \"position\": 0,\n",
    "        \"shouldSwap\": 0\n",
    "    })\n",
    "    f.step({\"drawFromRandom\": 1})\n",
    "    _, reward, *_ = f.step({\n",
    "        \"position\": 0,\n",
    "        \"shouldSwap\": 0\n",
    "    })\n",
    "\n",
    "    assert reward == -10, f\"Got reward {reward} but expected -10 when playing the same position twice\"\n",
    "\n",
    "\n",
    "test_invalid_position()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 23:03:41,855\tINFO worker.py:1518 -- Started a local Ray instance.\n",
      "2022-11-15 23:03:42,532\tINFO algorithm.py:1871 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "2022-11-15 23:03:42,533\tINFO ppo.py:378 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-11-15 23:03:42,534\tINFO algorithm.py:351 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/_private/ray_option_utils.py:266: DeprecationWarning: Setting 'object_store_memory' for actors is deprecated since it doesn't actually reserve the required object store memory. Use object spilling that's enabled by default (https://docs.ray.io/en/releases-2.0.0/ray-core/objects/object-spilling.html) instead to bypass the object store memory size limitation.\n",
      "  warnings.warn(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m 2022-11-15 23:03:44,999\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m 2022-11-15 23:03:45,026\tERROR worker.py:756 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=192390, ip=138.38.233.129, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fdfc676c490>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m   File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 613, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m   File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1784, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m   File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 123, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m     self[policy_id] = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m   File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 71, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m     return policy_class(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m   File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/algorithms/ppo/ppo_tf_policy.py\", line 83, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m     base.__init__(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m   File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/policy/dynamic_tf_policy_v2.py\", line 73, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m     self.dist_class = self._init_dist_class()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m   File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/policy/dynamic_tf_policy_v2.py\", line 406, in _init_dist_class\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m     dist_class, _ = ModelCatalog.get_action_dist(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m   File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 316, in get_action_dist\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m     return ModelCatalog._get_multi_action_distribution(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m   File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 977, in _get_multi_action_distribution\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m     child_dists_and_in_lens = tree.map_structure(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m   File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/tree/__init__.py\", line 430, in map_structure\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m     [func(*args) for args in zip(*map(flatten, structures))])\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m   File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/tree/__init__.py\", line 430, in <listcomp>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m     [func(*args) for args in zip(*map(flatten, structures))])\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m   File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 978, in <lambda>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m     lambda s: ModelCatalog.get_action_dist(s, config, framework=framework),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m   File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 340, in get_action_dist\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m     sum(action_space.nvec)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192390)\u001b[0m TypeError: iteration over a 0-d array\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m 2022-11-15 23:03:45,022\tERROR worker.py:756 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=192391, ip=138.38.233.129, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fadea18c520>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m   File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 613, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m   File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1784, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m   File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 123, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m     self[policy_id] = create_policy_for_framework(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m   File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 71, in create_policy_for_framework\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m     return policy_class(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m   File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/algorithms/ppo/ppo_tf_policy.py\", line 83, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m     base.__init__(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m   File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/policy/dynamic_tf_policy_v2.py\", line 73, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m     self.dist_class = self._init_dist_class()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m   File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/policy/dynamic_tf_policy_v2.py\", line 406, in _init_dist_class\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m     dist_class, _ = ModelCatalog.get_action_dist(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m   File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 316, in get_action_dist\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m     return ModelCatalog._get_multi_action_distribution(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m   File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 977, in _get_multi_action_distribution\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m     child_dists_and_in_lens = tree.map_structure(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m   File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/tree/__init__.py\", line 430, in map_structure\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m     [func(*args) for args in zip(*map(flatten, structures))])\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m   File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/tree/__init__.py\", line 430, in <listcomp>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m     [func(*args) for args in zip(*map(flatten, structures))])\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m   File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 978, in <lambda>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m     lambda s: ModelCatalog.get_action_dist(s, config, framework=framework),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m   File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 340, in get_action_dist\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m     sum(action_space.nvec)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=192391)\u001b[0m TypeError: iteration over a 0-d array\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:418\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 418\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers \u001b[39m=\u001b[39m WorkerSet(\n\u001b[1;32m    419\u001b[0m         env_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator,\n\u001b[1;32m    420\u001b[0m         validate_env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidate_env,\n\u001b[1;32m    421\u001b[0m         policy_class\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_default_policy_class(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig),\n\u001b[1;32m    422\u001b[0m         trainer_config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig,\n\u001b[1;32m    423\u001b[0m         num_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnum_workers\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    424\u001b[0m         local_worker\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    425\u001b[0m         logdir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogdir,\n\u001b[1;32m    426\u001b[0m     )\n\u001b[1;32m    427\u001b[0m \u001b[39m# WorkerSet creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[39m# be initialized properly (due to some errors in the RolloutWorker's\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[39m# constructor).\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:125\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_remote_workers \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 125\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_workers(\n\u001b[1;32m    126\u001b[0m     num_workers,\n\u001b[1;32m    127\u001b[0m     validate\u001b[39m=\u001b[39;49mtrainer_config\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mvalidate_workers_after_construction\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    128\u001b[0m )\n\u001b[1;32m    130\u001b[0m \u001b[39m# Create a local worker, if needed.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39m# the first remote worker (which does have an env).\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:269\u001b[0m, in \u001b[0;36mWorkerSet.add_workers\u001b[0;34m(self, num_workers, validate)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mif\u001b[39;00m validate:\n\u001b[0;32m--> 269\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforeach_worker(\u001b[39mlambda\u001b[39;49;00m w: w\u001b[39m.\u001b[39;49massert_healthy())\n",
      "File \u001b[0;32m~/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:391\u001b[0m, in \u001b[0;36mWorkerSet.foreach_worker\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    390\u001b[0m     local_result \u001b[39m=\u001b[39m [func(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_worker())]\n\u001b[0;32m--> 391\u001b[0m remote_results \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39;49mget([w\u001b[39m.\u001b[39;49mapply\u001b[39m.\u001b[39;49mremote(func) \u001b[39mfor\u001b[39;49;00m w \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mremote_workers()])\n\u001b[1;32m    392\u001b[0m \u001b[39mreturn\u001b[39;00m local_result \u001b[39m+\u001b[39m remote_results\n",
      "File \u001b[0;32m~/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(ray, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/_private/worker.py:2277\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2277\u001b[0m             \u001b[39mraise\u001b[39;00m value\n\u001b[1;32m   2279\u001b[0m \u001b[39mif\u001b[39;00m is_individual_id:\n",
      "\u001b[0;31mRayActorError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=192391, ip=138.38.233.129, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fadea18c520>)\n  File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 613, in __init__\n    self._build_policy_map(\n  File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1784, in _build_policy_map\n    self.policy_map.create_policy(\n  File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 123, in create_policy\n    self[policy_id] = create_policy_for_framework(\n  File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 71, in create_policy_for_framework\n    return policy_class(\n  File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/algorithms/ppo/ppo_tf_policy.py\", line 83, in __init__\n    base.__init__(\n  File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/policy/dynamic_tf_policy_v2.py\", line 73, in __init__\n    self.dist_class = self._init_dist_class()\n  File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/policy/dynamic_tf_policy_v2.py\", line 406, in _init_dist_class\n    dist_class, _ = ModelCatalog.get_action_dist(\n  File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 316, in get_action_dist\n    return ModelCatalog._get_multi_action_distribution(\n  File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 977, in _get_multi_action_distribution\n    child_dists_and_in_lens = tree.map_structure(\n  File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/tree/__init__.py\", line 430, in map_structure\n    [func(*args) for args in zip(*map(flatten, structures))])\n  File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/tree/__init__.py\", line 430, in <listcomp>\n    [func(*args) for args in zip(*map(flatten, structures))])\n  File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 978, in <lambda>\n    lambda s: ModelCatalog.get_action_dist(s, config, framework=framework),\n  File \"/home/ibraheem/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/models/catalog.py\", line 340, in get_action_dist\n    sum(action_space.nvec)\nTypeError: iteration over a 0-d array",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ibraheem/Desktop/Donut-bot/main.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ibraheem/Desktop/Donut-bot/main.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ray\u001b[39m.\u001b[39minit()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ibraheem/Desktop/Donut-bot/main.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m algo \u001b[39m=\u001b[39m ppo\u001b[39m.\u001b[39;49mPPO(env\u001b[39m=\u001b[39;49mSinglePlayerDonutEnv, config\u001b[39m=\u001b[39;49m{})\n",
      "File \u001b[0;32m~/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:308\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[39m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[39m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[39m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_metrics \u001b[39m=\u001b[39m {\n\u001b[1;32m    301\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevaluation\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m    302\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mepisode_reward_max\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m     }\n\u001b[1;32m    306\u001b[0m }\n\u001b[0;32m--> 308\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(config\u001b[39m=\u001b[39;49mconfig, logger_creator\u001b[39m=\u001b[39;49mlogger_creator, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    310\u001b[0m \u001b[39m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[39m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[39m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/tune/trainable/trainable.py:157\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, custom_syncer)\u001b[0m\n\u001b[1;32m    155\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_ip \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_current_ip()\n\u001b[0;32m--> 157\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig))\n\u001b[1;32m    158\u001b[0m setup_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m    159\u001b[0m \u001b[39mif\u001b[39;00m setup_time \u001b[39m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/miniconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:443\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[39mexcept\u001b[39;00m RayActorError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    431\u001b[0m     \u001b[39m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    432\u001b[0m     \u001b[39m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    433\u001b[0m     \u001b[39m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    434\u001b[0m     \u001b[39m# errors.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m     \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39mactor_init_failed:\n\u001b[1;32m    436\u001b[0m         \u001b[39m# Raise the original error here that the RolloutWorker raised\u001b[39;00m\n\u001b[1;32m    437\u001b[0m         \u001b[39m# during its construction process. This is to enforce transparency\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[39m# - e.args[0].args[2]: The original Exception (e.g. a ValueError due\u001b[39;00m\n\u001b[1;32m    442\u001b[0m         \u001b[39m# to a config mismatch) thrown inside the actor.\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m         \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs[\u001b[39m2\u001b[39m]\n\u001b[1;32m    444\u001b[0m     \u001b[39m# In any other case, raise the RayActorError as-is.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m         \u001b[39mraise\u001b[39;00m e\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d array"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ray.init()\n",
    "\n",
    "algo = ppo.PPO(env=SinglePlayerDonutEnv, config={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(algo.train())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('rllib')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Oct  7 2022, 20:19:58) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4696ef454f56e1321b5473c6d4b0148f4af87412dc47138694c27d2793afd11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
